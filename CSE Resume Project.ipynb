{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ebf52349-3294-4698-b783-ba72e43ff06d",
   "metadata": {},
   "source": [
    "# Applicant Tracking System: Resume and Job Description Matches\n",
    "\n",
    "*Replace this text with your summary, including research questions as a numbered list and answers for each research questions.*\n",
    "\n",
    "This is an applicant tracking system, often refered to as an ATS. They are commonly used by companies and businesses of all sizes, and they serve to sort through applicant resumes and return a subset as the best possible canidates for the job. Such \n",
    "\n",
    "1. What factors are the strongest indicator of a match score?\\\n",
    "**ANS:** The factor that most strongly indicates a match score was the degree a candidate held. The second most defining factor was the skills and candidate had, and the least defining was the responsibilities a candidate had in a previous position.\n",
    "\n",
    "3. Does direct word matching between a resume and description indicate a better match, or do synonyms perform just as well?\\\n",
    "**ANS:** Direct word matching between a resume and job description does indicate a better match. Synonym don’t seem to have any particular ordering, in terms of the best or worst synonym to use.\n",
    "\n",
    "5. How do different matching methods impact scoring, and what is the best method to get the most accurate score?\\\n",
    "**ANS:** The best method was to use Spacy’s similarity function to directly compare the text listed under each category, rather than using keyword extraction to uncover a percentage. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84e331f5-20f9-442c-ab65-364d4efb118c",
   "metadata": {},
   "source": [
    "## Challenge Goals\n",
    "- **New library:** Natural Language Processing: Resumes and job descriptions, by nature of their use, must be human focused and readable. But with the precedent set by companies like linkedin, indeed, and glassdoor, matches between job descriptions and resumes are needed on a much faster and larger scale, and natural language processing is especially helpful for this. Parsing out the meaning and synonyms of words is especially important for \n",
    "- **Multiple data sets:**  Resumes and job descriptions have a lot of room for variety, and multiple data sets can provide different types of resumes and job descriptions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcc3ea93-fc61-402a-bf04-490729b600d8",
   "metadata": {},
   "source": [
    "## Collaboration and Conduct\n",
    "\n",
    "Students are expected to follow Washington state law on the [Student Conduct Code for the University of Washington](https://www.washington.edu/admin/rules/policies/WAC/478-121TOC.html). In this course, students must:\n",
    "\n",
    "- Indicate on your submission any assistance received, including materials distributed in this course.\n",
    "- Not receive, generate, or otherwise acquire any substantial portion or walkthrough to an assessment.\n",
    "- Not aid, assist, attempt, or tolerate prohibited academic conduct in others.\n",
    "\n",
    "Update the following code cell to include your name and list your sources. If you used any kind of computer technology to help prepare your assessment submission, include the queries and/or prompts. Submitted work that is not consistent with sources may be subject to the student conduct process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "77c5d696-2e15-4c25-9fdf-b31e535b9c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "your_name = \"Ilse Schmitz\"\n",
    "sources = [\n",
    "    'https://blog.martianlogic.com/5-facts-you-must-know-about-the-applicant-tracking-system-ats' +\n",
    "    ' - This was used to uncover how ATS systems work',\n",
    "    'https://kristenfife.medium.com/understanding-how-the-ats-reads-and-interacts-with-your-resume-401bd00b66db' +\n",
    "    ' - This was used to uncover how ATS systems work',\n",
    "\n",
    "    'https://course.spacy.io/en - Spacys interactive learning course, I used it to undertand how spacy works,' +\n",
    "    'and what the spacy library can do for me.',\n",
    "    'https://spacy.io/usage/processing-pipelines' +\n",
    "    ' - FOnd by google searching \"python spacy pipeline API documentation\", I used this resource to better' +\n",
    "    'understand how to scale my code to work faster for large datasets',\n",
    "    \n",
    "    'https://deepnote.com/app/abid/spaCy-Resume-Analysis-81ba1e4b-7fa8-45fe-ac7a-0b7bf3da7826' +\n",
    "    ' - an incredible example of a working ATS system. No code has been directly copied from the example, but its' + \n",
    "    'a fantastic tool to see how ATS sysems work, and some of the ideas about how to identify keywords and' + \n",
    "    'compute percentages has been borrowed from this example.', \n",
    "    \n",
    "    'search.ipynb - Sourced the method \"clean()\" that was provided alongside this assignment', \n",
    "    'dataframes.ipynb'\n",
    "]\n",
    "\n",
    "assert your_name != \"\", \"your_name cannot be empty\"\n",
    "assert ... not in sources, \"sources should not include the placeholder ellipsis\"\n",
    "assert len(sources) >= 6, \"must include at least 6 sources, inclusive of lectures and sections\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3a9116b-b1c6-424a-a790-037dd3a8a55b",
   "metadata": {},
   "source": [
    "## Data Setting and Methods\n",
    "\n",
    "*Replace this text with a description of the data setting, any data transformations you conducted, and the methods you plan to use to answer the research questions. You may remove the code cell below if you don't need it.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17015230-171b-453f-b740-d54ab8f20b61",
   "metadata": {},
   "source": [
    "### Setting Up SpaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0032a573-c2ae-4495-8f0e-618276818dff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in c:\\users\\user\\downloads\\python3\\lib\\site-packages (3.8.4)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\user\\downloads\\python3\\lib\\site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\user\\downloads\\python3\\lib\\site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\user\\downloads\\python3\\lib\\site-packages (from spacy) (1.0.12)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\user\\downloads\\python3\\lib\\site-packages (from spacy) (2.0.11)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\user\\downloads\\python3\\lib\\site-packages (from spacy) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in c:\\users\\user\\downloads\\python3\\lib\\site-packages (from spacy) (8.3.4)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\user\\downloads\\python3\\lib\\site-packages (from spacy) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\user\\downloads\\python3\\lib\\site-packages (from spacy) (2.5.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\user\\downloads\\python3\\lib\\site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in c:\\users\\user\\downloads\\python3\\lib\\site-packages (from spacy) (0.4.1)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in c:\\users\\user\\downloads\\python3\\lib\\site-packages (from spacy) (0.15.2)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\user\\downloads\\python3\\lib\\site-packages (from spacy) (4.66.4)\n",
      "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\user\\downloads\\python3\\lib\\site-packages (from spacy) (1.26.4)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\user\\downloads\\python3\\lib\\site-packages (from spacy) (2.32.2)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\user\\downloads\\python3\\lib\\site-packages (from spacy) (2.5.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\user\\downloads\\python3\\lib\\site-packages (from spacy) (3.1.4)\n",
      "Requirement already satisfied: setuptools in c:\\users\\user\\downloads\\python3\\lib\\site-packages (from spacy) (69.5.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\user\\downloads\\python3\\lib\\site-packages (from spacy) (23.2)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\user\\downloads\\python3\\lib\\site-packages (from spacy) (3.5.0)\n",
      "Requirement already satisfied: language-data>=1.2 in c:\\users\\user\\downloads\\python3\\lib\\site-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\user\\downloads\\python3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.14.6 in c:\\users\\user\\downloads\\python3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.14.6)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in c:\\users\\user\\downloads\\python3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\user\\downloads\\python3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\user\\downloads\\python3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\user\\downloads\\python3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.20)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\user\\downloads\\python3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.8.30)\n",
      "Requirement already satisfied: blis<1.3.0,>=1.2.0 in c:\\users\\user\\downloads\\python3\\lib\\site-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.2.0)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\user\\downloads\\python3\\lib\\site-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
      "Requirement already satisfied: colorama in c:\\users\\user\\downloads\\python3\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.6)\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\users\\user\\downloads\\python3\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.7)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in c:\\users\\user\\downloads\\python3\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in c:\\users\\user\\downloads\\python3\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (13.3.5)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in c:\\users\\user\\downloads\\python3\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.21.0)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in c:\\users\\user\\downloads\\python3\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy) (5.2.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\user\\downloads\\python3\\lib\\site-packages (from jinja2->spacy) (2.1.3)\n",
      "Requirement already satisfied: marisa-trie>=1.1.0 in c:\\users\\user\\downloads\\python3\\lib\\site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
      "Requirement already satisfied: markdown-it-py<3.0.0,>=2.2.0 in c:\\users\\user\\downloads\\python3\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\user\\downloads\\python3\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\user\\downloads\\python3\\lib\\site-packages (from markdown-it-py<3.0.0,>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6a3aa769-44b1-414f-ae85-3d3f1f22270a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"<frozen runpy>\", line 189, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 148, in _get_module_details\n",
      "  File \"<frozen runpy>\", line 112, in _get_module_details\n",
      "  File \"C:\\Users\\User\\Downloads\\Python3\\Lib\\site-packages\\spacy\\__init__.py\", line 13, in <module>\n",
      "    from . import pipeline  # noqa: F401\n",
      "    ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\User\\Downloads\\Python3\\Lib\\site-packages\\spacy\\pipeline\\__init__.py\", line 1, in <module>\n",
      "    from .attributeruler import AttributeRuler\n",
      "  File \"C:\\Users\\User\\Downloads\\Python3\\Lib\\site-packages\\spacy\\pipeline\\attributeruler.py\", line 8, in <module>\n",
      "    from ..language import Language\n",
      "  File \"C:\\Users\\User\\Downloads\\Python3\\Lib\\site-packages\\spacy\\language.py\", line 46, in <module>\n",
      "    from .pipe_analysis import analyze_pipes, print_pipe_analysis, validate_attrs\n",
      "  File \"C:\\Users\\User\\Downloads\\Python3\\Lib\\site-packages\\spacy\\pipe_analysis.py\", line 6, in <module>\n",
      "    from .tokens import Doc, Span, Token\n",
      "  File \"C:\\Users\\User\\Downloads\\Python3\\Lib\\site-packages\\spacy\\tokens\\__init__.py\", line 1, in <module>\n",
      "    from ._serialize import DocBin\n",
      "  File \"C:\\Users\\User\\Downloads\\Python3\\Lib\\site-packages\\spacy\\tokens\\_serialize.py\", line 14, in <module>\n",
      "    from ..vocab import Vocab\n",
      "  File \"spacy\\vocab.pyx\", line 1, in init spacy.vocab\n",
      "  File \"spacy\\tokens\\doc.pyx\", line 49, in init spacy.tokens.doc\n",
      "  File \"C:\\Users\\User\\Downloads\\Python3\\Lib\\site-packages\\spacy\\schemas.py\", line 195, in <module>\n",
      "    class TokenPatternString(BaseModel):\n",
      "  File \"C:\\Users\\User\\Downloads\\Python3\\Lib\\site-packages\\pydantic\\v1\\main.py\", line 286, in __new__\n",
      "    cls.__try_update_forward_refs__()\n",
      "  File \"C:\\Users\\User\\Downloads\\Python3\\Lib\\site-packages\\pydantic\\v1\\main.py\", line 808, in __try_update_forward_refs__\n",
      "    update_model_forward_refs(cls, cls.__fields__.values(), cls.__config__.json_encoders, localns, (NameError,))\n",
      "  File \"C:\\Users\\User\\Downloads\\Python3\\Lib\\site-packages\\pydantic\\v1\\typing.py\", line 554, in update_model_forward_refs\n",
      "    update_field_forward_refs(f, globalns=globalns, localns=localns)\n",
      "  File \"C:\\Users\\User\\Downloads\\Python3\\Lib\\site-packages\\pydantic\\v1\\typing.py\", line 529, in update_field_forward_refs\n",
      "    update_field_forward_refs(sub_f, globalns=globalns, localns=localns)\n",
      "  File \"C:\\Users\\User\\Downloads\\Python3\\Lib\\site-packages\\pydantic\\v1\\typing.py\", line 520, in update_field_forward_refs\n",
      "    field.type_ = evaluate_forwardref(field.type_, globalns, localns or None)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\User\\Downloads\\Python3\\Lib\\site-packages\\pydantic\\v1\\typing.py\", line 66, in evaluate_forwardref\n",
      "    return cast(Any, type_)._evaluate(globalns, localns, set())\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "TypeError: ForwardRef._evaluate() missing 1 required keyword-only argument: 'recursive_guard'\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "afebcc79-82b1-4a64-b6be-ca3e91543395",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "ForwardRef._evaluate() missing 1 required keyword-only argument: 'recursive_guard'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjson\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mspacy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlanguage\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Language\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mre\u001b[39;00m\n",
      "File \u001b[1;32m~\\Downloads\\Python3\\Lib\\site-packages\\spacy\\__init__.py:13\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# These are imported as part of the API\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mthinc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Config, prefer_gpu, require_cpu, require_gpu  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pipeline  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m util\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mabout\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __version__  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n",
      "File \u001b[1;32m~\\Downloads\\Python3\\Lib\\site-packages\\spacy\\pipeline\\__init__.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mattributeruler\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AttributeRuler\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdep_parser\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DependencyParser\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01medit_tree_lemmatizer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m EditTreeLemmatizer\n",
      "File \u001b[1;32m~\\Downloads\\Python3\\Lib\\site-packages\\spacy\\pipeline\\attributeruler.py:8\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m util\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01merrors\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Errors\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlanguage\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Language\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmatcher\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Matcher\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mscorer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Scorer\n",
      "File \u001b[1;32m~\\Downloads\\Python3\\Lib\\site-packages\\spacy\\language.py:46\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlang\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtokenizer_exceptions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BASE_EXCEPTIONS, URL_MATCH\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlookups\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_lookups\n\u001b[1;32m---> 46\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpipe_analysis\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m analyze_pipes, print_pipe_analysis, validate_attrs\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mschemas\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     48\u001b[0m     ConfigSchema,\n\u001b[0;32m     49\u001b[0m     ConfigSchemaInit,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     52\u001b[0m     validate_init_settings,\n\u001b[0;32m     53\u001b[0m )\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mscorer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Scorer\n",
      "File \u001b[1;32m~\\Downloads\\Python3\\Lib\\site-packages\\spacy\\pipe_analysis.py:6\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mwasabi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m msg\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01merrors\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Errors\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtokens\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Doc, Span, Token\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dot_to_dict\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m TYPE_CHECKING:\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;66;03m# This lets us add type hints for mypy etc. without causing circular imports\u001b[39;00m\n",
      "File \u001b[1;32m~\\Downloads\\Python3\\Lib\\site-packages\\spacy\\tokens\\__init__.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_serialize\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DocBin\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdoc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Doc\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmorphanalysis\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MorphAnalysis\n",
      "File \u001b[1;32m~\\Downloads\\Python3\\Lib\\site-packages\\spacy\\tokens\\_serialize.py:14\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01merrors\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Errors\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SimpleFrozenList, ensure_path\n\u001b[1;32m---> 14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvocab\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Vocab\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_dict_proxies\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SpanGroups\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdoc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DOCBIN_ALL_ATTRS \u001b[38;5;28;01mas\u001b[39;00m ALL_ATTRS\n",
      "File \u001b[1;32m~\\Downloads\\Python3\\Lib\\site-packages\\spacy\\vocab.pyx:1\u001b[0m, in \u001b[0;36minit spacy.vocab\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\Downloads\\Python3\\Lib\\site-packages\\spacy\\tokens\\doc.pyx:49\u001b[0m, in \u001b[0;36minit spacy.tokens.doc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\Downloads\\Python3\\Lib\\site-packages\\spacy\\schemas.py:195\u001b[0m\n\u001b[0;32m    191\u001b[0m         obj \u001b[38;5;241m=\u001b[39m converted\n\u001b[0;32m    192\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m validate(TokenPatternSchema, {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpattern\u001b[39m\u001b[38;5;124m\"\u001b[39m: obj})\n\u001b[1;32m--> 195\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mTokenPatternString\u001b[39;00m(BaseModel):\n\u001b[0;32m    196\u001b[0m     REGEX: Optional[Union[StrictStr, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTokenPatternString\u001b[39m\u001b[38;5;124m\"\u001b[39m]] \u001b[38;5;241m=\u001b[39m Field(\u001b[38;5;28;01mNone\u001b[39;00m, alias\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mregex\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    197\u001b[0m     IN: Optional[List[StrictStr]] \u001b[38;5;241m=\u001b[39m Field(\u001b[38;5;28;01mNone\u001b[39;00m, alias\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124min\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\Downloads\\Python3\\Lib\\site-packages\\pydantic\\v1\\main.py:286\u001b[0m, in \u001b[0;36mModelMetaclass.__new__\u001b[1;34m(mcs, name, bases, namespace, **kwargs)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m__signature__ \u001b[38;5;241m=\u001b[39m ClassAttribute(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__signature__\u001b[39m\u001b[38;5;124m'\u001b[39m, generate_model_signature(\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m, fields, config))\n\u001b[0;32m    285\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m resolve_forward_refs:\n\u001b[1;32m--> 286\u001b[0m     \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m__try_update_forward_refs__()\n\u001b[0;32m    288\u001b[0m \u001b[38;5;66;03m# preserve `__set_name__` protocol defined in https://peps.python.org/pep-0487\u001b[39;00m\n\u001b[0;32m    289\u001b[0m \u001b[38;5;66;03m# for attributes not in `new_namespace` (e.g. private attributes)\u001b[39;00m\n\u001b[0;32m    290\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, obj \u001b[38;5;129;01min\u001b[39;00m namespace\u001b[38;5;241m.\u001b[39mitems():\n",
      "File \u001b[1;32m~\\Downloads\\Python3\\Lib\\site-packages\\pydantic\\v1\\main.py:808\u001b[0m, in \u001b[0;36mBaseModel.__try_update_forward_refs__\u001b[1;34m(cls, **localns)\u001b[0m\n\u001b[0;32m    802\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[0;32m    803\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__try_update_forward_refs__\u001b[39m(\u001b[38;5;28mcls\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mlocalns: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    804\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    805\u001b[0m \u001b[38;5;124;03m    Same as update_forward_refs but will not raise exception\u001b[39;00m\n\u001b[0;32m    806\u001b[0m \u001b[38;5;124;03m    when forward references are not defined.\u001b[39;00m\n\u001b[0;32m    807\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 808\u001b[0m     update_model_forward_refs(\u001b[38;5;28mcls\u001b[39m, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m__fields__\u001b[38;5;241m.\u001b[39mvalues(), \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m__config__\u001b[38;5;241m.\u001b[39mjson_encoders, localns, (\u001b[38;5;167;01mNameError\u001b[39;00m,))\n",
      "File \u001b[1;32m~\\Downloads\\Python3\\Lib\\site-packages\\pydantic\\v1\\typing.py:554\u001b[0m, in \u001b[0;36mupdate_model_forward_refs\u001b[1;34m(model, fields, json_encoders, localns, exc_to_suppress)\u001b[0m\n\u001b[0;32m    552\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m fields:\n\u001b[0;32m    553\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 554\u001b[0m         update_field_forward_refs(f, globalns\u001b[38;5;241m=\u001b[39mglobalns, localns\u001b[38;5;241m=\u001b[39mlocalns)\n\u001b[0;32m    555\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m exc_to_suppress:\n\u001b[0;32m    556\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "File \u001b[1;32m~\\Downloads\\Python3\\Lib\\site-packages\\pydantic\\v1\\typing.py:529\u001b[0m, in \u001b[0;36mupdate_field_forward_refs\u001b[1;34m(field, globalns, localns)\u001b[0m\n\u001b[0;32m    527\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m field\u001b[38;5;241m.\u001b[39msub_fields:\n\u001b[0;32m    528\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m sub_f \u001b[38;5;129;01min\u001b[39;00m field\u001b[38;5;241m.\u001b[39msub_fields:\n\u001b[1;32m--> 529\u001b[0m         update_field_forward_refs(sub_f, globalns\u001b[38;5;241m=\u001b[39mglobalns, localns\u001b[38;5;241m=\u001b[39mlocalns)\n\u001b[0;32m    531\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m field\u001b[38;5;241m.\u001b[39mdiscriminator_key \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    532\u001b[0m     field\u001b[38;5;241m.\u001b[39mprepare_discriminated_union_sub_fields()\n",
      "File \u001b[1;32m~\\Downloads\\Python3\\Lib\\site-packages\\pydantic\\v1\\typing.py:520\u001b[0m, in \u001b[0;36mupdate_field_forward_refs\u001b[1;34m(field, globalns, localns)\u001b[0m\n\u001b[0;32m    518\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m field\u001b[38;5;241m.\u001b[39mtype_\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m \u001b[38;5;241m==\u001b[39m ForwardRef:\n\u001b[0;32m    519\u001b[0m     prepare \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m--> 520\u001b[0m     field\u001b[38;5;241m.\u001b[39mtype_ \u001b[38;5;241m=\u001b[39m evaluate_forwardref(field\u001b[38;5;241m.\u001b[39mtype_, globalns, localns \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    521\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m field\u001b[38;5;241m.\u001b[39mouter_type_\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m \u001b[38;5;241m==\u001b[39m ForwardRef:\n\u001b[0;32m    522\u001b[0m     prepare \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32m~\\Downloads\\Python3\\Lib\\site-packages\\pydantic\\v1\\typing.py:66\u001b[0m, in \u001b[0;36mevaluate_forwardref\u001b[1;34m(type_, globalns, localns)\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mevaluate_forwardref\u001b[39m(type_: ForwardRef, globalns: Any, localns: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m     64\u001b[0m     \u001b[38;5;66;03m# Even though it is the right signature for python 3.9, mypy complains with\u001b[39;00m\n\u001b[0;32m     65\u001b[0m     \u001b[38;5;66;03m# `error: Too many arguments for \"_evaluate\" of \"ForwardRef\"` hence the cast...\u001b[39;00m\n\u001b[1;32m---> 66\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(Any, type_)\u001b[38;5;241m.\u001b[39m_evaluate(globalns, localns, \u001b[38;5;28mset\u001b[39m())\n",
      "\u001b[1;31mTypeError\u001b[0m: ForwardRef._evaluate() missing 1 required keyword-only argument: 'recursive_guard'"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from spacy.language import Language\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "import spacy\n",
    "from spacy.language import Language\n",
    "# from spacy.pipeline import EntityRuler\n",
    "from spacy.lang.en import English\n",
    "from spacy.tokens import Doc, Span"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c381262-f778-4af0-9464-a021e6d12bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_md')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79340afc-d9a3-4bc2-b907-6e905bedfdff",
   "metadata": {},
   "source": [
    "### Getting CSV files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01162c2b-7d97-4712-84b0-af9f24763c79",
   "metadata": {},
   "source": [
    "This is a dataset containing resume and job data, as well as a match score indicating the how well suited a resume is for each job. The '﻿job_position_name' column has been renamed, due to a red dot appearing only under certain circumstances. It appears in the code, but not within the displayed markdown text. I'm unsure why this appears or what it is, but the column has been renamed to avoid potential confusion or corruption. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c1ec53a-4857-480a-a63b-3fbdab571cd4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # Resume Job Match Dataset\n",
    "resume_job_match = pd.read_csv('resume_data.csv') \n",
    "resume_job_match.rename(columns={'﻿job_position_name': 'job_position_name'}, inplace=True)\n",
    "resume_job_match.fillna('', inplace=True)\n",
    "cols = ['skills', 'skills_required', 'related_skils_in_job', 'responsibilities', 'responsibilities.1']\n",
    "for col in cols:\n",
    "    resume_job_match[col] =  resume_job_match[col].str.replace('\\n', ' ')\n",
    "resume_job_match"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cf8b2ea-c6c4-4dcd-815d-d14bc156cff4",
   "metadata": {},
   "source": [
    "### Build Matcher"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f82369a-4b1d-4c5e-aaf6-b747b453a157",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2692b0de-71c3-4d79-906a-dcfa63a27e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_docs(dataframe, disabled, col_A, col_B=None):\n",
    "    \"\"\"\n",
    "    Takes in a dataframe, a list of strings containg the pipes to disable when creating nlp objects, and one or two | *****\n",
    "    strings containing the names of columns in the data frame.     \n",
    "    If there are two column names given, concatenate each element found in col_A and col_B element wise into a list \n",
    "    of the same length as the dataframe.\n",
    "    Otherwise, transform the column col_A from the dataframe into a list. \n",
    "    Transform each list item into a generator of Doc items.\n",
    "    Return the generator.\n",
    "    \"\"\"\n",
    "    if col_B is None:\n",
    "        col_data = dataframe[col_A].tolist()\n",
    "    else:\n",
    "        col_data = (dataframe[col_A] + dataframe[col_B]).tolist()\n",
    "    docs = nlp.pipe(col_data, disable=disabled)\n",
    "    return docs\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb4696c-9e5a-4781-9f19-c4c577a85232",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def check_similarity(data, job_col_name, res_col_name, res2_col_name=None):\n",
    "    \"\"\"\n",
    "    Takes in a dataframe containing resume and job description data, and two or three strings containing the names of \n",
    "    columns found within the dataframe. \n",
    "    If two column names are given, make the third column name a None object. \n",
    "    ...\n",
    "    Find the similarity between the resume data and the job description data.\n",
    "    Return a list with the score of similarity between the resume data and the job data, and two generators containing\n",
    "    Doc objects with the natural-language-processed resume data and job data, respectivly\n",
    "    \"\"\"\n",
    "    skill_match = []\n",
    "    disabled = ['tokenizer', 'tagger', 'parser', 'lemmatizer', 'textcat', 'custom']\n",
    "\n",
    "    job_docs = create_docs(data, disabled, job_col_name)\n",
    "    res_docs = create_docs(data, disabled, res_col_name, res2_col_name)\n",
    "\n",
    "    for res_doc, job_doc in zip(res_docs, job_docs):\n",
    "        if (job_doc.text == '') or (res_doc.text == ''):\n",
    "            score = None\n",
    "        else:\n",
    "            score = res_doc.similarity(job_doc)\n",
    "        skill_match.append(score)\n",
    "\n",
    "    return skill_match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b29c91e9-b6b3-47d4-ab76-c1a0b6b96fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_sq_err(compute, true):\n",
    "    \"\"\"\n",
    "    Takes in two lists containing computed values and true values respectivly, and computes the mean squared error.\n",
    "    If there is a missing value in either list, skip to the next computed value and true value \n",
    "    \"\"\"\n",
    "    MSE = 0\n",
    "    for i in range(min(len(compute), len(true))):\n",
    "        if (compute[i] is not None) and (true[i] is not None):\n",
    "            MSE += np.square(np.subtract(compute[i], true[i]))\n",
    "    \n",
    "    return (MSE / (i+1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf6cb48f-4229-4502-b542-e26ea91378ab",
   "metadata": {},
   "source": [
    "### Research Question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf33191-6c7c-4e59-b722-04dcd025cfec",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['skills_required', 'skills', 'related_skils_in_job', \n",
    "          'educationaL_requirements', 'degree_names', 'major_field_of_studies',\n",
    "         'responsibilities.1', 'responsibilities', None,\n",
    "         'job_position_name', 'positions', None]\n",
    "title = ['skills', 'degree', 'responsibilities', 'job position']\n",
    "\n",
    "match_score = resume_job_match['matched_score'].tolist()\n",
    "\n",
    "similarity = dict()\n",
    "for i in range(int(len(columns) / 3)):\n",
    "    sim = check_similarity(resume_job_match, columns[3*i], columns[3*i + 1], columns[3*i + 2])\n",
    "    MSE = mean_sq_err(sim, match_score)\n",
    "    similarity[title[i]] = MSE\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d52b9d-f4fe-48ae-a31d-9a93d498e972",
   "metadata": {},
   "source": [
    "To explain how these numbers are calculated, it's important to explain where this information comes from. USing skills as an example, I began with the data from *'resume_data.csv'*. I calculated the similarity between the skills the applicant had listed, the skills relevant to the job position the applicant worked previously, and the skills required by the job description. I combined the data provided by the applicant into a single string, and checked the similarity between the applicant's skills and the skills listed in the job description. A match between 0 and 1 is provided, with 1 being a perfect match, and 0 being completely unrelated. This is the computed match.\\\n",
    "For some categories, like the previous job positions held by the applicant, only one string is provided. As such, the computed value can take one or two strings of applicant information, depending on whether applicant information is stored in one or two places. \n",
    "From there, I computed the mean squared error. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad33046f-f9ef-40bf-aea6-cd5d89f48e72",
   "metadata": {},
   "source": [
    "The mean squared error is computed between the match score provided by the dataset, and the computed values for each category of information. In this case, the mean squared error is used to indicate how heavily an ATS may weigh each section when calculating the match score. The larger the mean squared error, the less weight that section holds, and thus the less important it is to the ATS. Conversely, the smaller the mean squared error, the more important that section is. \n",
    "\n",
    "This is not a perfect estimate, by any means. In practice, an ATS would take the score provided for each category and compute the match score with different categories being weighted. For example, if the degree was less important than a candidate's previous job roles. However, we do not have the weights used to compute the provided match score. Further, when using several different categories of applicant information to evaluate, the data may heavily weight one particular category, but be much closer to several lesser weighted categories, and as such, mean squared error for each category would be high for the most heavily weighted, and much closer to the numerous lightly weighted categories. As such, discerning the category with the most importance placed upon from this information is tenuous at best, and outright wrong at worst.\n",
    "\n",
    "However, this information can tell us the best indicator for a match score. If the mean squared error between the skills scores is small, this can indicate that the similarity between an applicant's skills and the required skills is the best indicator for whether the match score will be high or low. This doesn't tell us the weights, but it does tell us the most likely outcome for a match. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c0fba54-a3bb-4b13-aa3b-b931fa06606f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(similarity)):\n",
    "    print(f\"Mean Squared error of {title[i]}: \", similarity[title[i]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f33cd5e3-bc47-402b-9c67-09228e2ecea6",
   "metadata": {},
   "source": [
    "The category with the smallest mean squared error is degree. With a score of approximately $0.0529$, this is possibly due to the brevity that a list of applicant's degrees would naturally have a better match score with a smaller list of words to compare, but this is surprising to see. Generally, it would be expected that prior positions and work experience would overrule this, but it's certainly not unfeasible that an applicant who had the degree required for the position would be prioritized in comparison to an applicant lacking the necessary credentials. \n",
    "This means that an applicant's degree and level of education are the best indicator for whether or not that person is a good match for the job. \n",
    "\n",
    "Shortly after the job position category comes the skills category with approximately $0.0718$, and after that, the job position category with $0.109$. \n",
    "\n",
    "In contrast, the mean squared error of the responsibilities category, $0.143$ , indicates that the degree is the least important indicator of a good resume-job-description match. Similar to the degree category, this is surprising. It would go to show that a candidate with previous experience carrying out the same or similar tasks would be a better fit for a job, but this could very well be an incorrect assumption. Further, it's important to remember that the computer is simply matching words, and as such, lacks the ability to pick up on transferable skills, and further doesn't have the ability to ask the candidate to expand upon topics within a job interview.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbbc7f00-908a-4ab7-8924-cd5b59c917d3",
   "metadata": {},
   "source": [
    "### Research Question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37af071e-7112-43cb-9e1c-57a1d92ffc5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This retains the mean squared error of the original skills score\n",
    "MSE_skills = similarity[title[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60fd0dba-888a-47b1-9529-63e45d886ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "synonym = ['machine learning', 'AI', 'neural network']\n",
    "synonym_df = []\n",
    "\n",
    "\n",
    "for i in range(len(synonym)):\n",
    "    df = resume_job_match[['skills', 'skills_required', 'related_skils_in_job']].copy()\n",
    "    df['skills'] = df['skills'].str.replace('Machine Learning', synonym[i])\n",
    "    df['skills'] = df['skills'].str.replace('ML', synonym[i])\n",
    "    \n",
    "    df['related_skils_in_job'] = df['related_skils_in_job'].str.replace('Machine Learning', synonym[i])\n",
    "    df['related_skils_in_job'] = df['related_skils_in_job'].str.replace('ML', synonym[i])\n",
    "    synonym_df.append(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7926a01-d8c4-49ff-92f0-2a898bab2bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "synonym_skills_compare = dict()\n",
    "for i in range(len(synonym_df)):\n",
    "    synonym_scores = check_similarity(synonym_df[i], 'skills', 'skills_required', 'related_skils_in_job')\n",
    "    MSE = mean_sq_err(synonym_scores, match_score)\n",
    "    synonym_skills_compare[synonym[i]] = MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a778dc-f5c2-46a0-b048-7881b54b6b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Mean Squared Error of \\'Machine Learning\\' or  \\'ML\\': \", MSE_skills)\n",
    "\n",
    "for i in range(len(synonym_skills_compare)):\n",
    "    print(f\"Mean Squared Error of \\'{synonym[i]}\\': \", synonym_skills_compare[synonym[i]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec005cac-9690-436d-b71e-19e719e6fb2d",
   "metadata": {},
   "source": [
    "This is the mean squared error for four different variations of the word \"Machine Learning\". This was carried out by finding every instance of the word \"Machine Learning\" or \"ML\" helps to see whether exact word matching has the same impact on word matching as synonyms. This tells us whether direct word matching works better, or whether synonyms work just as good. \n",
    "\n",
    "In this case, starting with the original dataset, we have the mean squared error, approximately $0.718$. For each instance of skills, each instance of \"Machine Learning\" and \"ML\" has been replaced with \"machine learning\" in all lowercase, \"AI\", or \"neural network\". These are not the same thing, but they are synonyms, and thus in testing out how closely related the mean squared error is, this can inform whether direct word matching is better than synonym matching for resume and job description checkers. \n",
    "\n",
    "In this case, all three examples of the synonyms had an increased mean squared error close to $0.098$, with an increase of just over $0.026$. This provides evidence to support that direct word matching would be the best way to go, rather than relying on synonyms or indirect word matches, as only the resumes were modified during this search. \n",
    "There is always the possibility the by changing to synonyms, thus leading to an increased match score, but given the regularity of increase, even when changing \"Machine Learning\" and \"ML\" to all lowercase, this indicates that any incidental increase is still heavily weighted out by the affects of indirect matching. \n",
    "\n",
    "Thus, it can be concluded that direct word matching when writing a resume is likely to be more effective at inciting a higher match score than using synonyms. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e532654-7b84-4433-934c-9ca8e5541e5f",
   "metadata": {},
   "source": [
    "### Research Question 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0fcd42a-9bc3-4951-8aac-01f341d26053",
   "metadata": {},
   "outputs": [],
   "source": [
    "ruler = nlp.add_pipe(\"entity_ruler\", after=\"ner\")\n",
    "\n",
    "patterns = []\n",
    "with open('jz_skill_patterns.jsonl', 'r') as f:\n",
    "    for line in f:\n",
    "        data = json.loads(line)\n",
    "        patterns.append(data)\n",
    "\n",
    "ruler.add_patterns(patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b5e4d8c-dc7f-4e45-9b2c-c84584a06b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(token: str, pattern: re.Pattern[str] = re.compile(r\"\\W+\")) -> str:\n",
    "    \"\"\"\n",
    "    Returns all the characters in the token lowercased and without matches to the\n",
    "    given pattern.\n",
    "\n",
    "    >>> clean(\"Hello!\")\n",
    "    'hello'\n",
    "    \"\"\"\n",
    "    return pattern.sub(\"\", token.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c3656a-6a93-47e7-b3c5-aeafde074521",
   "metadata": {},
   "outputs": [],
   "source": [
    "def keyword_compare(data, job_col_name, res_col_name, res2_col_name=None):\n",
    "    \"\"\"...\"\"\"\n",
    "    job_docs = create_docs(data, [], job_col_name)\n",
    "    res_docs = create_docs(data, [], res_col_name, res2_col_name)\n",
    "\n",
    "    job_skills = set()\n",
    "    res_skills = set()\n",
    "    match_per = []\n",
    "    \n",
    "    for res_doc, job_doc in zip(res_docs, job_docs):\n",
    "        total_matches = 0\n",
    "    \n",
    "        # If there are any skills listed in the job description, add them to a set \n",
    "        for ent in job_doc.ents:\n",
    "            job_skills.add(clean(ent.text))\n",
    "    \n",
    "        # If there are required skills, check the skills in the resume\n",
    "        # Otherwise, the number of matching skills is zero\n",
    "        if len(job_skills) > 0:\n",
    "            for ent in res_doc.ents:\n",
    "                res_skills.add(clean(ent.text))\n",
    "    \n",
    "        # If a skill in the job description is found in the resume, add one to total_matches \n",
    "        for skill in job_skills:\n",
    "            if skill in res_skills:\n",
    "                total_matches += 1\n",
    "                \n",
    "        if (len(job_skills) == 0):\n",
    "            # No skills required for job\n",
    "            match_per.append(None)\n",
    "        else:\n",
    "            match_per.append(total_matches / len(job_skills))\n",
    "\n",
    "    return match_per"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a697139-1a9f-4e55-a9a7-2a113d0c0534",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['skills_required', 'skills', 'related_skils_in_job', \n",
    "          'educationaL_requirements', 'degree_names', 'major_field_of_studies',\n",
    "         'responsibilities.1', 'responsibilities', None,\n",
    "         'job_position_name', 'positions', None]\n",
    "\n",
    "percentage = []\n",
    "\n",
    "for i in range(int(len(columns) / 3)):\n",
    "    percentage.append(keyword_compare(resume_job_match, columns[3*i], columns[3*i + 1], columns[3*i + 2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed9b339f-62fe-4221-9210-20f39a6c5494",
   "metadata": {},
   "source": [
    "Keyword selection, in this case, uses named entity recognition to pick out skills, organizations, and people. This may seem like a strange choice, and it certainly is an imperfect one, but further diving into the datasets provides answers as to why this choice was made. In particular, the jobzilla dataset used to identify skills is prioritized around identifying skills common in or adjacent to the tech industry, and thus skills like \"customer service\" are more difficult to identify. \n",
    "However, as we are measuring the percentage of the number of keywords in the job description being present in the resume and all keywords are being identified in the same manner, part of this discrepancy can be accounted for. If \"customer service\" cannot be identified by the keyword finder, it doesn't matter if it's present in the job description or the resume. Its presence or absence will not contribute to the match percentage. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0daf2f5-6edf-4021-9f4c-07abaedcc69c",
   "metadata": {},
   "outputs": [],
   "source": [
    "title = ['skills', 'degree', 'responsibilities', 'job position']\n",
    "for i in range(len(percentage)):\n",
    "    MSE_per = mean_sq_err(percentage[i], match_score)\n",
    "    print(f\"Mean Squared error of {title[i]} percentage: \", MSE_per)\n",
    "    print(f\"Mean Squared error of {title[i]} similarity: \", similarity[title[i]])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d057b1-8af5-4bfe-af4f-7583abed50fd",
   "metadata": {},
   "source": [
    "From the numbers computed, we can compare and see what way of reading resumes is better. Generally speaking, we can tell the mean squared error of keyword selecting is larger compared to the mean squared error using spacy's similarity function. This is likely due to the fact that spacy's similarity is Asofter on indirect matches, such as the difference between the phrases \"Neural Network\" and \"Machine Learning\". For spacy's similarity function, this would still produce a number indicating the closeness of each word, whereas the keyword matching would produce zero no matter what, since those words are not the same. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf1e527-dbaa-4ea9-88e3-d1fcb51b2192",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp1 = nlp(\"Neural Network\")\n",
    "nlp2 = nlp(\"Machine Learning\")\n",
    "\n",
    "print(nlp1.similarity(nlp1))\n",
    "print(nlp1.similarity(nlp2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5bf3cdb-1dcc-40a5-ba94-cc2dd3cb51f9",
   "metadata": {},
   "source": [
    "Ultimately, the answer of what gives the most accurate score goes to spacy's similarity function. The smaller mean squared error is a clear indicator that it's providing a better read across three of the four categories. The only exception to this was the responsibilities category, where both keyword matching and similarity produced the same result, approximately $0.1429$. This is bizarre, more so than if the keyword percentage was smaller than the similarity number. This could indicate that spacy's similarity function uses some of the same methods as the keyword method, such as utilizing a percentage. It could also be possible that the numbers are similar only when truncated, but given that both numbers go out to seventeen decimal places, the fact that both numbers have the same digits that far out is genuinely baffling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "345fd7a0-fdd5-482f-88ad-c0c6dc25ec79",
   "metadata": {},
   "source": [
    "This also provides the answer to the final part of research question 3. Of the methods presented here, the most accurate scoring would be to use spacy's built in similarity function, though there is certainly room for improvement. \n",
    "\n",
    "Lastly, this helps support the second research question, albeit indirectly. When taking a holistic approach to ATS's in daily life, it's generally unknown what methods an ATS could be using, whether it be using an in-house similarity computation, an open-source option like spacy, or a more complicated AI reader. In any case, it stands to reason that all three options would find the best matches with direct word matching, rather than synonyms, even when synonyms are accounted for. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b3957de-7724-43ad-b50c-b3aef5738f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "del resume_job_match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "665f8c29-e6b4-4448-a491-085df1aff23a",
   "metadata": {},
   "outputs": [],
   "source": [
    "resume_data = pd.read_csv('test.csv') \n",
    "height, width = resume_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb37321-a3cf-432a-be98-a771983ba4a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_data = pd.read_csv('jts.csv') \n",
    "job_short = job_data.head(height).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27bb086f-dfd4-4724-a490-e39f26287dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "match2 = pd.concat([resume_data, job_short], axis=1)\n",
    "del job_data\n",
    "del job_short\n",
    "del resume_data\n",
    "match2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afbb315b-34b2-4a19-8a67-7b8f96c24754",
   "metadata": {},
   "source": [
    "## Implications and Limitations\n",
    "\n",
    "A wide variety of people would benefit from utilizing this analysis, including job applicants, recruiters, HR personnel, or anyone wishing to build their own ATS. Ultimately, it's an introductory step into the variety of applicant tracking systems. It provides some information on how they can be best utilized, and given the growing number of companies using them, it's especially unlikely that they're going away. Indeed, for many reading this, it's especially likely that their resumes will be graded by an ATS. Knowing how they work and how to structure a resume so that it's both human and machine readable is an important skill to have. \n",
    "\n",
    "However, it would be remiss to talk about how ATS's work, and not talk about how they, like all projects that rely on data, are only as good as the data they're given. When talking about employment especially, there is plenty of discrimination that shows up in hiring records, and even attempts to mitigate bias can still yield pitiful results. One especially notable example of this was Amazon's AI recruiting tool was found to be biased against hiring women, and was found to systematically discriminate against people using the word \"women\" within their resume. Even when the word \"women\" was explicitly removed from consideration, the tool still suffered from bias problems, and was ultimately scrapped. In fact, this plays into one of the limitations of this tool. Despite the fact that names, genders, and ages are not included within the datasets, things like extracurricular activities, what college someone attended, where they worked previously, or even how they phrase skills, responsibilities, or accomplishments can be used to extrapolate identifying information, and given that this dataset does not include this information, it's difficult to tell where bias is being reinforced. \n",
    "\n",
    "Another limitation of this project was that it was built using imperfect data. In particular, despite having a large number of records, the skills dataset in particular was preferential to include terms common in the tech industry, rather than having a balanced variety of terms. The fact that changing certain terms still leads to significant changes in the data does indicate the relevance of the terms, but the noticeable prevalence does indicate that the data is not necessarily the most reliable. \n",
    "\n",
    "Lastly, while this covered two potential ways an ATS may work, ATS's vary widely depending on whose programming them, or how they're programmed to run. One of the most common ways is to utilize machine learning to measure the similarity between a resume and a job description, and is an aspect left uncovered by this project. Ultimately, this was decided due to debugging constraints, and learning to integrate natural language processing into neural networks was a topic deserving of its own deep dive, so for ease, this was scrapped from this project. However, this is leaving out one of the most common elements in an ATS out of a project specifically about exploring them. \n",
    "\n",
    "Ultimately, this is one source of information on a varied and complex topic, and should be treated as such. Like any good researcher, to find out more, getting information from multiple sources is important to reaching a well-rounded understanding. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
